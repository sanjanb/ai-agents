"""
Simple RAG memory agent using SentenceTransformers and FAISS.

Builds a tiny index over sample texts and answers a query with retrieved context.

Run:
  python examples/agents/rag_memory_agent.py
"""

from __future__ import annotations

import sys
from typing import List, Tuple

import numpy as np

try:
    import faiss  # type: ignore
except Exception:
    raise SystemExit("faiss-cpu is required. Install requirements first.")

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    raise SystemExit("sentence-transformers is required. Install requirements first.")


def build_index(texts: List[str], model_name: str = "all-MiniLM-L6-v2"):
    model = SentenceTransformer(model_name)
    embs = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
    embs = embs.astype("float32")
    index = faiss.IndexFlatIP(embs.shape[1])
    index.add(embs)
    return model, index


def search(query: str, model: SentenceTransformer, index, texts: List[str], k: int = 3) -> List[Tuple[str, float]]:
    q = model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")
    scores, ids = index.search(q, k)
    results = []
    for s, i in zip(scores[0], ids[0]):
        if 0 <= i < len(texts):
            results.append((texts[i], float(s)))
    return results


def answer_with_context(query: str, context: List[str]) -> str:
    # Minimal stub: concatenate context; in real systems, call an LLM here.
    return (
        "Context:\n" + "\n---\n".join(context) + f"\n\nQuestion: {query}\n\n(Answer would be generated by an LLM here.)"
    )


def main() -> None:
    texts = [
        "Machine learning is a subset of AI that enables computers to learn from data.",
        "Deep learning uses neural networks with multiple layers to model complex patterns.",
        "Reinforcement learning trains agents by rewarding desired behaviors and punishing undesired ones.",
        "Vector databases store embeddings and enable fast similarity search for retrieval-augmented generation.",
        "Retrieval-Augmented Generation (RAG) reduces hallucinations by grounding LLM outputs in retrieved documents.",
    ]

    model, index = build_index(texts)
    query = "How does RAG help with hallucinations?"
    results = search(query, model, index, texts, k=3)
    context = [r[0] for r in results]
    print(answer_with_context(query, context))


if __name__ == "main__":  # pragma: no cover
    main()

# For direct execution
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.exit(0)
